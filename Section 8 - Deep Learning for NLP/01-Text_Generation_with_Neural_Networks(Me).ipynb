{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE FUNCTION THAT CAN READ FILES\n",
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZE AND CLEAN THE TEXT\n",
    "import spacy \n",
    "\n",
    "# We only need Spacy for Tokenization\n",
    "#nlp = spacy.load('en', disable = ['parser', 'tagger', 'ner'])\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['parser', 'tagger', 'ner'])\n",
    "\n",
    "# Here we only use up to chapter 4 of moby dick (not all)\n",
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea is I'm going to read in this entire thing as a string, pass it in to NLP and then iterate through the tokens grabbing their text and then lowercasing it. <br>\n",
    "However, I wanna get rid of things that are probably not gonna be very helpful for training purposes, like periods or new lines, because they show up so often in this actual text,\n",
    "especially new lines, that I wanna make sure my text generation neural network doesn't overfit to that sort of punctuation. <br>\n",
    "Otherwise, you may just get a bunch of periods or a bunch of new lines at the end since those are common enough that the neural network overfits to them. <br>\n",
    "We're really interested in the relationship between words\n",
    "\n",
    "Common Punctuation provided by CARIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\anaconda3\\envs\\ml_nlp\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "tokens = seperate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11338"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, what we're going to do is create sequence of tokens. <br>\n",
    "Basically Running the Model with 24 Words of a Sentence, and trying to predict the 25th Word. <br>\n",
    "The Number of words that you want are dependent on the Document you want to Predict. <br>\n",
    "You want the model to to grab the structure of a sentence but not short enough where you're missing general context. <br>\n",
    "For example Song Lyrics could be shorter, and Shakespeare could be more than 50 words <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 Words --> Network Predict # 26\n",
    "\n",
    "train_len = 25 + 1\n",
    "\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    seq = tokens[i - train_len:i]   # i - train_len all the way to i\n",
    "\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore i'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "Replace Text to Unique Numbers / ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# Lots of parameters that you can put in Tokenizer\n",
    "\n",
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Text to Unique Numbers / ID\n",
    "#sequences[0]\n",
    "#sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in sequences[0]:\n",
    "#    print(f\"{i} : {tokenizer.index_word[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now the Type of Sequences is a list where every item in the list is another list of these actual numbers. <br>\n",
    "What I'd like to do is format that to be a numPy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these rows represents a single line in the text. <br>\n",
    "Notice how we're essentially shifting one word over (956 14 263) the next one (14, 263, 51) <br>\n",
    "<br>\n",
    "So given these ID numbers for each word what is the expected word to come after those first 25 words?<br>\n",
    "So we already have our features here as well as our label and later on we'll be performing a train test splitwith that functionality<br>\n",
    "which is why we kind of needed it in this numPy array.<br>\n",
    "<br>\n",
    "Basically 25 + 1 = 26 <br>\n",
    "25 are the Features, 1 Label <br>\n",
    "The 25 Are being Trained to Predict the 1 Label <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ..., 2713,   14,   24],\n",
       "       [  14,  263,   51, ...,   14,   24,  957],\n",
       "       [ 263,   51,  261, ...,   24,  957,    5],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,  262,   53,    2],\n",
       "       [  12,  166, 2712, ...,   53,    2, 2718],\n",
       "       [ 166, 2712,    3, ...,    2, 2718,   26]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Label Split <br>\n",
    "because there's nothing to test against. <br>\n",
    "There's kind of no right answer as far as what text generated should look like instead, we are really just texting or testing these features against the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ...,    6, 2713,   14],\n",
       "       [  14,  263,   51, ..., 2713,   14,   24],\n",
       "       [ 263,   51,  261, ...,   14,   24,  957],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,   11,  262,   53],\n",
       "       [  12,  166, 2712, ...,  262,   53,    2],\n",
       "       [ 166, 2712,    3, ...,   53,    2, 2718]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEATURES\n",
    "# Grab for every row, all the columns EXPECT the very last column\n",
    "X = sequences[:, :-1]   # row, columns\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  24,  957,    5, ...,    2, 2718,   26])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LABELS\n",
    "# Grab all the rows AND JUST grabbed the last column\n",
    "y = sequences[: , -1]   # ro, columns\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes = vocabulary_size + 1)    # The way Padding works, it need extra one to hold a zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences_Length <br> \n",
    "11,368 Sequences. Those are essentially the shifted 25 words sentences <br>\n",
    "And in Each Sentences there are 25 Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11312, 25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = X.shape[1]\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create The Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll be importing a Dense Layer and LSTM Layer to deal with the sequences <br>\n",
    "and an Embedding Layer to deal with the vocabulary <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # EMBEDDING (See Notebook)\n",
    "    # Check Parameter Desc -> (input_dim, output_dim, embeddings_initializer)\n",
    "    model.add(Embedding(vocabulary_size, seq_len, input_length = seq_len))\n",
    "\n",
    "    # LSTM Layers\n",
    "    # For Neurons usually it is best to have 2x of Sequence Length \n",
    "    model.add(LSTM(50, return_sequences = True))\n",
    "    model.add(LSTM(50))\n",
    "\n",
    "    # Dense Layer\n",
    "    # Here 50 Neuron is just to Make the Training Quicker, real Training should be HIGHER\n",
    "    model.add(Dense(50, activation = 'relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            67975     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 50)            15200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2719)              138669    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244,594\n",
      "Trainable params: 244,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# +1 is to hold the extra zero\n",
    "model = create_model(vocabulary_size + 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Fit Our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load the File Later on\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch size is how many sequences you want to pass in at a time. You don't want to pass in everything at a time, <br>\n",
    "otherwise the the neural network won't be able to handle that so you only want to pass in a certain amount of sequences, <br>\n",
    "now 128 was a value that I kind of just chose arbitrarily and it worked well for me. <br>\n",
    "<br>\n",
    "Two epoch is not going to be nearly enough to generate any text that makes sense, so you may just see a bunch of the most common words <br>\n",
    "like the, the,  the repeated but go ahead and train it on a little bit of epoch just so you can tell that it worked or not <br>\n",
    "You should probably be training for at least like 200 epoch to get something that's reasonable. <br>\n",
    "<br>\n",
    "and then verbose one, it's just going to be the output report. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "89/89 [==============================] - 14s 75ms/step - loss: 6.9966 - accuracy: 0.0393\n",
      "Epoch 2/2\n",
      "89/89 [==============================] - 7s 76ms/step - loss: 6.3780 - accuracy: 0.0529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e02d356fb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size = 128, epochs = 2, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our accuracy is absolutely horrible which makes sense. <br>\n",
    "Right now it's probably just predicting the word to be the most common word <br>\n",
    "but once this is done training,especially if you trained for a really long time, <br>\n",
    "you're going to want to save this. <br>\n",
    "<br>\n",
    "the other thing we're actually going to save is the tokenizer as well <br>\n",
    "Remember that tokenizer has information across the entire vocabulary like word counts, <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODEL\n",
    "model.save('my_mobydick_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE TOKENIZER\n",
    "dump(tokenizer, open('my_simpletokenizer', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "What's next is to actually generate new text. <br>\n",
    "So what we're gonna do is we're going to create a function that generates new text for us based off a given model, <br>\n",
    "tokenizer, sequence length, a seed text and then the number of words to be generated by the model. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we actually need to feed it some sort of line of 25 tokens that we wanna start off with and then it's gonna generate one word after that. <br>\n",
    "Then we're what gonna do is chop off the very first word of the seed, take in our new word, put it at the end, <br>\n",
    "and then we have our new ctext or our new input text after that. <br>\n",
    "And then we're gonna keep doing that however many times the user wants to generate words. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we're gonna pass in our input text. And we're gonna grab the first item here because it basically returns a tuple or a list. <br>\n",
    "<br>\n",
    "we're gonna pad sequences to our trained rate. since we only trained on 25 tokens, we're gonna pad it to make sure it's only 25 tokens. <br>\n",
    "Or if your ctext happens to be too short, then we're gonna pad it to fill up the 25 spaces.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "\n",
    "    output_text = []\n",
    "\n",
    "    input_text = seed_text\n",
    "\n",
    "    for i in range(num_gen_words):\n",
    "\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "\n",
    "        # Read pad_sequences Parameters Description\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen = seq_len, truncating = 'pre')\n",
    "\n",
    "        # Predict Class Probabilities for Each Word\n",
    "        pred_word_ind = model.predict_classes(pad_encoded, verbose = 0)[0]\n",
    "\n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "\n",
    "        input_text += ' '+pred_word\n",
    "\n",
    "        output_text.append(pred_word)\n",
    "\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is our generate text function doing? <br>\n",
    "It's taking in the Model we just trained, the Tokenizer, which has knowledge about the vocabulary and what ID number goes with what word, <br>\n",
    "the Sequence Length, some Seed Text you wanna start off with. And then the Number Of Words we wanna generate.<br>\n",
    "<br>\n",
    "And then let's say I wanna generate 10 words. So for I in range number of words, so I'm gonna do this 10 times, wanna generate 10 words <br>\n",
    "I'm going to first take the input text string and encode it to be a sequence. <br>\n",
    "Essentially, what we did earlier, we transformed those raw text data into sequences of numbers. <br>\n",
    "Then if my ctext happens to be too short or too long, I may need to pad it, I may need to cut it off or I may need to add to it. (pad_encoded Function) <br>\n",
    "<br>\n",
    "After that, I'm going to predict the class probabilities for each word. So a model that predict classes is essentially going to throughout the entire vocabulary, <br>\n",
    "assign a probability to the most likely next word. <br>\n",
    "Next, we're gonna have the actual predicted word. So the way predict classes works when we index it with a zero, it's gonna return the index of that particular word. <br>\n",
    "Essentially its ID which if we can call tokenizer index word from before, we just pass in that index and it matches with the actual word. <br>\n",
    "<br>\n",
    "Then we're gonna take in the input text and I'm going to add a space and then add on that predicted word. So if my input text in the very beginning was 25 words, <br>\n",
    "after running this here for the first loop or the first pass on this for loop, it's now gonna be 26 words. Which means I'm then going to pad it. And that's why I'm gonna truncate with pre here. So it chops off the very first word. <br>\n",
    "So essentially creating sequences as it goes along, but more and more, the sequenceis gonna be my predicted words. And if you make number generated words long enough <br>\n",
    "eventually, you'll just be predicting on your own predicted words. <br>\n",
    "<br>\n",
    "So true generation without even any seed. Well, there is always a seed but after you do this enough times, if your number of generated words is longer than your ctext number of words, \n",
    "then you'll be predicting off your predicted words. <br>\n",
    "Now we still wanna actually append that predicted word. So we'll say the output text and we'll append the predicted word. <br>\n",
    "So this input text is for prediction purposes. This output text is all I'm actually gonna show. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(101)\n",
    "random_pick = random.randint(0,len(text_sequences))\n",
    "\n",
    "random_seed_text = text_sequences[random_pick]\n",
    "\n",
    "#random_seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thought i to myself the man 's a human being just as i am he has just as much reason to fear me as i have\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = ' '.join(random_seed_text)\n",
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_text\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_gen_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 15\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(model, tokenizer, seq_len, seed_text, num_gen_words)\u001b[0m\n\u001b[0;32m     12\u001b[0m pad_encoded \u001b[38;5;241m=\u001b[39m pad_sequences([encoded_text], maxlen \u001b[38;5;241m=\u001b[39m seq_len, truncating \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Predict Class Probabilities for Each Word\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m pred_word_ind \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_classes\u001b[49m(pad_encoded, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     17\u001b[0m pred_word \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mindex_word[pred_word_ind]\n\u001b[0;32m     19\u001b[0m input_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpred_word\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "generate_text(model, tokenizer, seq_len, seed_text = seed_text, num_gen_words = 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
